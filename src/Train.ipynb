{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import encoder, model\n",
    "import json\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk.data\n",
    "import scipy.special"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "models_dir = '../models'\n",
    "model_name = '117M'\n",
    "data_path = '~/Downloads/stanfordSentimentTreebank'\n",
    "batch_size = 1\n",
    "seed = 231654\n",
    "max_len = 140\n",
    "num_classes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model\n",
    "models_dir = os.path.expanduser(os.path.expandvars(models_dir))\n",
    "data_path = os.path.expanduser(os.path.expandvars(data_path))\n",
    "enc = encoder.get_encoder(model_name, models_dir)\n",
    "hparams = model.default_hparams()\n",
    "with open(os.path.join(models_dir, model_name, 'hparams.json')) as f:\n",
    "    hparams.override_from_dict(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "dataset_path = os.path.join(data_path, 'datasetSentences.txt')\n",
    "label_path = os.path.join(data_path, 'sentiment_labels.txt')\n",
    "split_path = os.path.join(data_path, 'datasetSplit.txt')\n",
    "dictionary_path = os.path.join(data_path, 'dictionary.txt')\n",
    "sentences = pd.read_csv(dataset_path, sep='\\t')\n",
    "labels = pd.read_csv(label_path, sep='|')\n",
    "splits = pd.read_csv(split_path, sep=',')\n",
    "dictionary = {}\n",
    "with open(dictionary_path) as fin:\n",
    "    for line in fin.readlines():\n",
    "        tokens = line.strip().split('|')\n",
    "        dictionary[tokens[0]] = tokens[1]\n",
    "sentences['phrase ids'] = sentences.apply(lambda x: int(dictionary.get(x['sentence'], 0)), axis=1)\n",
    "dataset = sentences.merge(labels).merge(splits)\n",
    "\n",
    "def score_sentiment(sentiment_score):\n",
    "    if sentiment_score > 0.8:\n",
    "        return 4\n",
    "    elif sentiment_score > 0.6:\n",
    "        return 3\n",
    "    elif sentiment_score > 0.4:\n",
    "        return 2\n",
    "    elif sentiment_score > 0.2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "dataset['sentiment label'] = dataset.apply(lambda x: score_sentiment(x['sentiment values']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X [[3646, 592, 588, 257, 2089, 13516, 286, 281, 625, 380, 431, 4471, 286, 3195, 705, 82, 32801, 705, 82, 13509, 290, 257, 32099, 290, 13526, 276, 12, 2902, 2196, 286, 5896, 8362, 764, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220]]\n",
      "Y [0]\n",
      "iteration 0 accuracy 0.0 cost 191.32114 running time 3\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "\n",
    "def get_batch_generator(splitset_label, batch_size):\n",
    "    def pad(encoding):\n",
    "        padlen = max_len-len(encoding)\n",
    "        encoding.extend([220]*padlen)\n",
    "        return encoding\n",
    "    \n",
    "    \n",
    "    while True:\n",
    "        dataset_ = dataset[dataset['splitset_label']==splitset_label]\n",
    "        random_subset = dataset_.sample(batch_size)\n",
    "        X = random_subset['sentence'].apply(lambda x: pad(enc.encode(x)[:40])).tolist()\n",
    "        y = random_subset['sentiment label'].to_numpy()\n",
    "        yield X, y\n",
    "\n",
    "def get_ordered_batch_generator(splitset_label, batch_size):\n",
    "    def pad(encoding):\n",
    "        padlen = max_len-len(encoding)\n",
    "        encoding.extend([220]*padlen)\n",
    "        return encoding\n",
    "    dataset_ = dataset[dataset['splitset_label']==splitset_label]\n",
    "    for i in range(0, dataset_.shape[0]-batch_size, batch_size):\n",
    "        batch_ = dataset.iloc[i:i+batch_size]\n",
    "        X = batch_['sentence'].apply(lambda x: pad(enc.encode(x)[:40])).tolist()\n",
    "        y = batch_['sentiment label'].to_numpy()\n",
    "        yield X, y\n",
    "        \n",
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "    context = tf.placeholder(tf.int32, [batch_size, max_len])\n",
    "    y = tf.placeholder(tf.int32, [batch_size])\n",
    "    np.random.seed(seed)\n",
    "    tf.set_random_seed(seed)\n",
    "    \n",
    "    # Model\n",
    "    classifier = model.model(hparams=hparams, X=context, past=None, reuse=tf.AUTO_REUSE)\n",
    "    \n",
    "    with tf.variable_scope('classification_head', reuse=tf.AUTO_REUSE):\n",
    "        logits = classifier['logits']\n",
    "        dropout = tf.nn.dropout(logits, keep_prob=0.9)\n",
    "        fc_2 = tf.layers.dense(dropout, 5)\n",
    "        avg_logits = tf.math.reduce_mean(fc_2, axis=1)\n",
    "        classifier['avg_logits'] = avg_logits\n",
    "    \n",
    "    # Cost\n",
    "    cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=classifier['avg_logits']))\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(\n",
    "            tf.argmax(classifier['avg_logits'], 1, output_type = tf.int32), y\n",
    "        ), tf.float32))\n",
    "    \n",
    "    init_all = tf.initializers.global_variables()\n",
    "    sess.run(init_all)\n",
    "    \n",
    "    saver = tf.train.Saver([v for v in tf.trainable_variables() if 'model' in v.name])\n",
    "    ckpt = tf.train.latest_checkpoint(os.path.join(models_dir, model_name))\n",
    "    saver.restore(sess, ckpt)\n",
    "    \n",
    "    # Train\n",
    "    start_time = time.time()\n",
    "    random_batch_generator = get_batch_generator(1, batch_size)\n",
    "    for i in range(1):\n",
    "        X, Y = next(random_batch_generator)\n",
    "        print ('X', X)\n",
    "        print ('Y', Y)\n",
    "        acc, cst, _ = sess.run([accuracy, cost, optimizer],feed_dict={context: X, y: Y})\n",
    "        if(i % 100 == 0):\n",
    "            print('iteration', i, 'accuracy', acc, 'cost', cst, 'running time', int(time.time()-start_time))\n",
    "            \n",
    "    # Test\n",
    "    run_test = False\n",
    "    if run_test:\n",
    "        ordered_batch_generator = get_ordered_batch_generator(2, batch_size)\n",
    "        accuracies = []\n",
    "        costs = []\n",
    "        while True:\n",
    "            try:\n",
    "                X, Y = next(ordered_batch_generator)\n",
    "                acc, cst = sess.run([accuracy, cost], feed_dict={context: X, y: Y})\n",
    "                accuracies.append(acc)\n",
    "                costs.append(cst)\n",
    "            except:\n",
    "                break\n",
    "        print(\"accuracy\", np.mean(np.array(accuracies)))\n",
    "        print(\"cost\", np.mean(np.array(costs)))\n",
    "\n",
    "    # Save model\n",
    "    #saver = tf.train.Saver()\n",
    "    #saver.save(sess, os.path.join(models_dir, model_name+\"sa_140\"))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "def get_encoded_sentence(text):\n",
    "    def pad(encoding):\n",
    "        padlen = max_len-len(encoding)\n",
    "        encoding.extend([220]*padlen)\n",
    "        return encoding\n",
    "    X = []\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    for sentence in tokenizer.tokenize(text):\n",
    "        encoding = enc.encode(sentence)\n",
    "        X.append(pad(enc.encode(sentence)[:max_len])[:max_len])\n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    context = tf.placeholder(tf.int32, [1, max_len])\n",
    "    # Model\n",
    "    classifier = model.model(hparams=hparams, X=context, past=None, reuse=tf.AUTO_REUSE)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, os.path.join(models_dir, model_name+\"sa_140\"))\n",
    "    logits = classifier['avg_logits']\n",
    "    scores = logits\n",
    "    inp = get_encoded_sentence(\"I doubt this company will go anywhere but bankrupt.\")\n",
    "    tot_score = np.zeros(5)\n",
    "    for sentence in inp:\n",
    "        sentence_score = sess.run([scores],feed_dict={context: [sentence]})[0].reshape(-1)\n",
    "        tot_score += sentence_score\n",
    "        \n",
    "    tot_score = scipy.special.softmax(tot_score)\n",
    "    xs = np.arange(5)\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.bar(xs, tot_score)\n",
    "    plt.xticks(xs, ('Very Negative', 'Negative', 'Neutral', 'Positive', 'Very Positive'))\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\"I think adoption will be slow, and I think Tesla is the least efficient manufacturer and will get buried in the next few years.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
